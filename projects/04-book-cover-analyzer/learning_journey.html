<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Book Cover Analyzer: Complete Learning Journey</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }

        .header h1 {
            font-size: 48px;
            margin-bottom: 20px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }

        .header p {
            font-size: 20px;
            opacity: 0.95;
        }

        .checkpoint-badge {
            display: inline-block;
            background: rgba(255,255,255,0.2);
            padding: 10px 20px;
            border-radius: 25px;
            margin-top: 20px;
            font-weight: bold;
            border: 2px solid rgba(255,255,255,0.3);
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 60px;
            border-left: 5px solid #667eea;
            padding-left: 30px;
        }

        .section-title {
            font-size: 32px;
            color: #667eea;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .section-number {
            background: #667eea;
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 24px;
        }

        .concept-box {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }

        .concept-title {
            font-size: 24px;
            color: #333;
            margin-bottom: 15px;
            font-weight: bold;
        }

        .concept-subtitle {
            font-size: 14px;
            color: #666;
            margin-bottom: 15px;
            font-style: italic;
        }

        .terms-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .term-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
            transition: all 0.3s ease;
        }

        .term-card:hover {
            border-color: #667eea;
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.2);
        }

        .term-name {
            font-weight: bold;
            color: #667eea;
            font-size: 18px;
            margin-bottom: 10px;
        }

        .term-description {
            color: #555;
            font-size: 14px;
        }

        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            overflow-x: auto;
            margin: 20px 0;
            line-height: 1.5;
        }

        .code-comment {
            color: #5c6370;
        }

        .code-keyword {
            color: #c678dd;
        }

        .code-function {
            color: #61afef;
        }

        .code-string {
            color: #98c379;
        }

        .what-you-learned {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 25px;
            border-radius: 12px;
            margin: 20px 0;
        }

        .what-you-learned h3 {
            margin-bottom: 15px;
            font-size: 22px;
        }

        .what-you-learned ul {
            list-style: none;
            padding-left: 0;
        }

        .what-you-learned li {
            padding: 8px 0;
            padding-left: 25px;
            position: relative;
        }

        .what-you-learned li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            font-weight: bold;
        }

        .image-placeholder {
            background: linear-gradient(45deg, #f0f0f0 25%, transparent 25%, transparent 75%, #f0f0f0 75%, #f0f0f0),
                        linear-gradient(45deg, #f0f0f0 25%, transparent 25%, transparent 75%, #f0f0f0 75%, #f0f0f0);
            background-size: 20px 20px;
            background-position: 0 0, 10px 10px;
            border: 2px dashed #ccc;
            border-radius: 10px;
            padding: 40px;
            text-align: center;
            color: #666;
            margin: 20px 0;
            font-style: italic;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .comparison-table th {
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: bold;
        }

        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e0e0e0;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        .pro {
            color: #28a745;
            font-weight: bold;
        }

        .con {
            color: #dc3545;
            font-weight: bold;
        }

        .key-insight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .key-insight-title {
            font-weight: bold;
            color: #856404;
            margin-bottom: 10px;
            font-size: 18px;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
            padding-top: 30px;
            border-top: 2px solid #e0e0e0;
        }

        .nav-button {
            background: #667eea;
            color: white;
            padding: 12px 30px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
            display: inline-block;
        }

        .nav-button:hover {
            background: #764ba2;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .nav-button.disabled {
            background: #ccc;
            cursor: not-allowed;
        }

        .timeline {
            position: relative;
            padding-left: 40px;
            margin: 30px 0;
        }

        .timeline:before {
            content: '';
            position: absolute;
            left: 15px;
            top: 0;
            bottom: 0;
            width: 3px;
            background: #667eea;
        }

        .timeline-item {
            position: relative;
            margin-bottom: 30px;
        }

        .timeline-item:before {
            content: '';
            position: absolute;
            left: -33px;
            top: 5px;
            width: 15px;
            height: 15px;
            border-radius: 50%;
            background: #667eea;
            border: 3px solid white;
            box-shadow: 0 0 0 3px #667eea;
        }

        .results-box {
            background: #e8f5e9;
            border: 2px solid #4caf50;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
        }

        .results-title {
            color: #2e7d32;
            font-weight: bold;
            font-size: 20px;
            margin-bottom: 15px;
        }

        .stat-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }

        .stat-item {
            background: white;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }

        .stat-number {
            font-size: 36px;
            font-weight: bold;
            color: #667eea;
        }

        .stat-label {
            color: #666;
            font-size: 14px;
            margin-top: 5px;
        }

        .stage-badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: bold;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- HEADER -->
        <div class="header">
            <h1>üìö Book Cover Analyzer</h1>
            <p>A Complete Learning Journey from Classical Computer Vision to Deep Learning OCR</p>
            <div class="checkpoint-badge">üìç Checkpoint A: Learning Distillation</div>
        </div>

        <!-- CONTENT -->
        <div class="content">

            <!-- INTRODUCTION -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">üéØ</div>
                    <span>What We Built & Why</span>
                </div>

                <div class="concept-box">
                    <p style="font-size: 18px; line-height: 1.8;">
                        <strong>The Goal:</strong> Build a system that can analyze a book cover image and automatically identify:
                    </p>
                    <ul style="margin-left: 40px; margin-top: 15px; font-size: 16px;">
                        <li>Where the book is in the image</li>
                        <li>What text is on the cover</li>
                        <li>What images/graphics are present</li>
                        <li>What the text actually says (OCR)</li>
                        <li>What each element means (Title? Author? Publisher?)</li>
                    </ul>

                    <div class="key-insight" style="margin-top: 20px;">
                        <div class="key-insight-title">üí° The Learning Path</div>
                        We took TWO different approaches to solve this problem, learning fundamental concepts along the way:
                        <br><br>
                        <strong>Approach 1 (Manual Detection):</strong> Classical computer vision techniques - building everything from scratch<br>
                        <strong>Approach 2 (OCR-Based):</strong> Deep learning approach - using pre-trained neural networks
                    </div>
                </div>
            </div>

            <!-- STAGE 1: FINDING THE BOOK -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">1</div>
                    <span>Stage 1: Finding the Book Cover</span>
                </div>

                <div class="stage-badge">COMMON TO BOTH APPROACHES</div>

                <div class="concept-box">
                    <div class="concept-title">üìê Concept: Edge Detection & Contour Finding</div>
                    <div class="concept-subtitle">How do we find the book's boundary in an image?</div>

                    <p><strong>The Problem:</strong> The image contains a book on a background. We need to isolate just the book.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Use edge detection to find boundaries, then identify the largest rectangular shape.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">Grayscale Conversion</div>
                            <div class="term-description">
                                Convert color image (BGR) to grayscale (single channel). Edges are easier to detect in grayscale.
                                <br><br>
                                <strong>Why?</strong> Color adds complexity. Edges are about intensity changes, not color.
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Gaussian Blur</div>
                            <div class="term-description">
                                Smooth the image to remove noise before edge detection.
                                <br><br>
                                <strong>Kernel size: (5, 5)</strong> - larger kernel = more blur
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Canny Edge Detection</div>
                            <div class="term-description">
                                Finds edges by detecting rapid changes in pixel intensity (gradients).
                                <br><br>
                                <strong>Parameters:</strong>
                                <ul style="margin-top: 5px; font-size: 13px;">
                                    <li>Low threshold: 50</li>
                                    <li>High threshold: 150</li>
                                </ul>
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Contour Detection</div>
                            <div class="term-description">
                                Find continuous curves/boundaries in the edge image.
                                <br><br>
                                <strong>RETR_EXTERNAL:</strong> Only outer contours<br>
                                <strong>CHAIN_APPROX_SIMPLE:</strong> Compress contours
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Polygon Approximation</div>
                            <div class="term-description">
                                Simplify contour to approximate polygon (find corners).
                                <br><br>
                                <strong>approxPolyDP:</strong> Douglas-Peucker algorithm<br>
                                <strong>Epsilon: 0.02 * perimeter</strong>
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Aspect Ratio</div>
                            <div class="term-description">
                                Width / Height ratio to validate shape.
                                <br><br>
                                <strong>Books:</strong> Typically 0.4 - 1.2<br>
                                Too wide or tall = probably not a book
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Step 1: Convert to grayscale</span>
gray = <span class="code-function">cv2.cvtColor</span>(image, cv2.COLOR_BGR2GRAY)

<span class="code-comment"># Step 2: Blur to remove noise</span>
blurred = <span class="code-function">cv2.GaussianBlur</span>(gray, (5, 5), 0)

<span class="code-comment"># Step 3: Find edges using Canny</span>
edged = <span class="code-function">cv2.Canny</span>(blurred, 50, 150)

<span class="code-comment"># Step 4: Find contours (shapes)</span>
contours = <span class="code-function">cv2.findContours</span>(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

<span class="code-comment"># Step 5: Sort by area (largest first)</span>
contours = <span class="code-keyword">sorted</span>(contours, key=cv2.contourArea, reverse=<span class="code-keyword">True</span>)

<span class="code-comment"># Step 6: Find book (large + rectangular + ~4 corners)</span>
<span class="code-keyword">for</span> contour <span class="code-keyword">in</span> contours:
    peri = <span class="code-function">cv2.arcLength</span>(contour, <span class="code-keyword">True</span>)
    approx = <span class="code-function">cv2.approxPolyDP</span>(contour, 0.02 * peri, <span class="code-keyword">True</span>)

    <span class="code-keyword">if</span> <span class="code-function">len</span>(approx) == 4:  <span class="code-comment"># 4 corners = rectangle</span>
        book_contour = contour
        <span class="code-keyword">break</span>
                    </div>

                    <div class="image-placeholder">
                        üì∑ Image 1: Edge Detection Result
                        <br>Shows white edges on black background - book outline is visible
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Color Spaces:</strong> BGR vs Grayscale - when and why to convert</li>
                            <li><strong>Noise Reduction:</strong> Gaussian blur smooths images before processing</li>
                            <li><strong>Edge Detection:</strong> Canny algorithm finds boundaries via gradients</li>
                            <li><strong>Contours:</strong> Continuous curves representing object boundaries</li>
                            <li><strong>Shape Analysis:</strong> Area, perimeter, approximation to polygons</li>
                            <li><strong>Geometric Validation:</strong> Aspect ratio checks for expected shapes</li>
                        </ul>
                    </div>
                </div>

                <!-- Perspective Transform -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üîÑ Concept: Perspective Transform</div>
                    <div class="concept-subtitle">Straightening tilted/rotated books to "bird's eye view"</div>

                    <p><strong>The Problem:</strong> Book might be tilted, rotated, or viewed at an angle.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Use 4-point perspective transform to unwarp the image.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">Point Ordering</div>
                            <div class="term-description">
                                Order 4 corners clockwise: top-left, top-right, bottom-right, bottom-left.
                                <br><br>
                                <strong>Method:</strong> Use sum and difference of coordinates
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Transformation Matrix</div>
                            <div class="term-description">
                                3x3 matrix that maps source points to destination points.
                                <br><br>
                                <strong>getPerspectiveTransform:</strong> Calculates this matrix
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Warping</div>
                            <div class="term-description">
                                Apply transformation to entire image.
                                <br><br>
                                <strong>warpPerspective:</strong> Transforms image using matrix
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Destination Size</div>
                            <div class="term-description">
                                Calculate output image dimensions using Euclidean distance.
                                <br><br>
                                Width = max(top_width, bottom_width)<br>
                                Height = max(left_height, right_height)
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Step 1: Order the 4 corner points</span>
<span class="code-keyword">def</span> <span class="code-function">order_points</span>(pts):
    rect = np.<span class="code-function">zeros</span>((4, 2), dtype=<span class="code-string">"float32"</span>)

    <span class="code-comment"># Top-left: smallest sum (x+y)</span>
    <span class="code-comment"># Bottom-right: largest sum</span>
    s = pts.<span class="code-function">sum</span>(axis=1)
    rect[0] = pts[np.<span class="code-function">argmin</span>(s)]  <span class="code-comment"># Top-left</span>
    rect[2] = pts[np.<span class="code-function">argmax</span>(s)]  <span class="code-comment"># Bottom-right</span>

    <span class="code-comment"># Top-right: smallest difference (y-x)</span>
    <span class="code-comment"># Bottom-left: largest difference</span>
    diff = np.<span class="code-function">diff</span>(pts, axis=1)
    rect[1] = pts[np.<span class="code-function">argmin</span>(diff)]  <span class="code-comment"># Top-right</span>
    rect[3] = pts[np.<span class="code-function">argmax</span>(diff)]  <span class="code-comment"># Bottom-left</span>

    <span class="code-keyword">return</span> rect

<span class="code-comment"># Step 2: Calculate output dimensions</span>
maxWidth = <span class="code-function">max</span>(
    np.<span class="code-function">sqrt</span>((br[0]-bl[0])**2 + (br[1]-bl[1])**2),  <span class="code-comment"># Bottom width</span>
    np.<span class="code-function">sqrt</span>((tr[0]-tl[0])**2 + (tr[1]-tl[1])**2)   <span class="code-comment"># Top width</span>
)

<span class="code-comment"># Step 3: Create transformation matrix</span>
M = <span class="code-function">cv2.getPerspectiveTransform</span>(src_points, dst_points)

<span class="code-comment"># Step 4: Apply transform</span>
warped = <span class="code-function">cv2.warpPerspective</span>(image, M, (maxWidth, maxHeight))
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Geometric Transforms:</strong> Perspective vs affine transformations</li>
                            <li><strong>Coordinate Systems:</strong> Image coordinates, point ordering</li>
                            <li><strong>Euclidean Distance:</strong> sqrt((x2-x1)¬≤ + (y2-y1)¬≤)</li>
                            <li><strong>Transformation Matrices:</strong> 3x3 homography matrices</li>
                            <li><strong>Document Scanning:</strong> Real-world application (straightening docs)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- DIVERGENCE POINT -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">‚ö°</div>
                    <span>The Two Approaches Diverge</span>
                </div>

                <div class="key-insight">
                    <div class="key-insight-title">üîÄ Critical Decision Point</div>
                    <p>After extracting the book cover, we have TWO ways to proceed:</p>
                    <br>
                    <strong>Path A: Manual Detection (Classical CV)</strong> - Build everything from scratch using computer vision techniques
                    <br><br>
                    <strong>Path B: OCR-First (Deep Learning)</strong> - Use pre-trained neural networks to detect and read text
                    <br><br>
                    We explored BOTH paths to understand the trade-offs!
                </div>

                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Manual Detection (Path A)</th>
                            <th>OCR-First (Path B)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Approach</strong></td>
                            <td>Build detection from scratch using edges, contours, thresholding</td>
                            <td>Use EasyOCR neural network to detect and read text</td>
                        </tr>
                        <tr>
                            <td><strong>Learning Value</strong></td>
                            <td><span class="pro">‚úì</span> Understand HOW computer vision works</td>
                            <td><span class="pro">‚úì</span> Learn to use modern AI tools</td>
                        </tr>
                        <tr>
                            <td><strong>Complexity</strong></td>
                            <td>High - many steps, many parameters to tune</td>
                            <td>Low - one function call does detection + reading</td>
                        </tr>
                        <tr>
                            <td><strong>Results</strong></td>
                            <td><span class="con">‚úó</span> Splits words into letters ("T", "E", "A", "M")</td>
                            <td><span class="pro">‚úì</span> Keeps words together ("TEAM")</td>
                        </tr>
                        <tr>
                            <td><strong>Control</strong></td>
                            <td><span class="pro">‚úì</span> Full control over every step</td>
                            <td><span class="con">‚úó</span> Black box - limited control</td>
                        </tr>
                        <tr>
                            <td><strong>Best For</strong></td>
                            <td>Learning fundamentals, custom requirements</td>
                            <td>Production systems, accuracy, speed</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- PATH A: MANUAL DETECTION -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">2A</div>
                    <span>Path A: Manual Detection (Classical CV)</span>
                </div>

                <div class="stage-badge">APPROACH 1: BUILD FROM SCRATCH</div>

                <!-- Adaptive Thresholding -->
                <div class="concept-box">
                    <div class="concept-title">‚ö´‚ö™ Concept: Adaptive Thresholding</div>
                    <div class="concept-subtitle">Converting images to black-and-white to isolate text</div>

                    <p><strong>The Problem:</strong> Text appears dark on a light background. We need to separate them.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Calculate thresholds locally (per region) instead of globally.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">Binary Image</div>
                            <div class="term-description">
                                Image with only 2 values: 0 (black) or 255 (white).
                                <br><br>
                                Makes analysis simpler - text vs background
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Global Threshold</div>
                            <div class="term-description">
                                One threshold value for entire image.
                                <br><br>
                                <strong>Problem:</strong> Fails with varying lighting
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Adaptive Threshold</div>
                            <div class="term-description">
                                Different threshold for each region (local calculation).
                                <br><br>
                                <strong>ADAPTIVE_THRESH_GAUSSIAN_C:</strong> Weighted average
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Block Size</div>
                            <div class="term-description">
                                Size of neighborhood for local threshold calculation.
                                <br><br>
                                <strong>15x15:</strong> Larger = smoother thresholding
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Constant C</div>
                            <div class="term-description">
                                Value subtracted from calculated threshold.
                                <br><br>
                                Fine-tunes sensitivity (higher = more white)
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">THRESH_BINARY_INV</div>
                            <div class="term-description">
                                Inverse: Dark becomes white, light becomes black.
                                <br><br>
                                We want text (dark) to become white (foreground)
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Adaptive thresholding - handles varying lighting</span>
thresh = <span class="code-function">cv2.adaptiveThreshold</span>(
    blurred,                              <span class="code-comment"># Input (grayscale)</span>
    255,                                  <span class="code-comment"># Max value (white)</span>
    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,       <span class="code-comment"># Method: Gaussian weighted</span>
    cv2.THRESH_BINARY_INV,                <span class="code-comment"># Inverse: dark ‚Üí white</span>
    15,                                   <span class="code-comment"># Block size (neighborhood)</span>
    3                                     <span class="code-comment"># Constant C</span>
)

<span class="code-comment"># Result: Black background, white text/images</span>
                    </div>

                    <div class="image-placeholder">
                        üì∑ Image 2: Adaptive Thresholding Result
                        <br>Black-and-white image showing text as white on black background
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Binarization:</strong> Converting grayscale to black-and-white</li>
                            <li><strong>Local vs Global:</strong> Why adaptive thresholding handles lighting better</li>
                            <li><strong>Neighborhood Operations:</strong> Sliding window calculations</li>
                            <li><strong>Inverse Thresholding:</strong> When and why to invert</li>
                            <li><strong>Parameter Tuning:</strong> Block size and constant C effects</li>
                        </ul>
                    </div>
                </div>

                <!-- Morphological Operations -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üîß Concept: Morphological Operations</div>
                    <div class="concept-subtitle">Cleaning up binary images - connecting text, removing noise</div>

                    <p><strong>The Problem:</strong> Thresholded image has broken text and small noise spots.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Use dilation to connect, erosion to shrink.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">Structuring Element (Kernel)</div>
                            <div class="term-description">
                                Small matrix that defines the operation's shape.
                                <br><br>
                                <strong>MORPH_RECT (5x5):</strong> Rectangular 5x5 kernel
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Dilation</div>
                            <div class="term-description">
                                Expands white regions (grows foreground).
                                <br><br>
                                <strong>Use:</strong> Connect broken text, fill gaps
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Erosion</div>
                            <div class="term-description">
                                Shrinks white regions (erodes foreground).
                                <br><br>
                                <strong>Use:</strong> Remove small noise, separate touching objects
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Iterations</div>
                            <div class="term-description">
                                Number of times to repeat the operation.
                                <br><br>
                                More iterations = stronger effect
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Opening</div>
                            <div class="term-description">
                                Erosion followed by dilation.
                                <br><br>
                                Removes small white noise while preserving shape
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Closing</div>
                            <div class="term-description">
                                Dilation followed by erosion.
                                <br><br>
                                Fills small holes while preserving shape
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Create kernel (structuring element)</span>
kernel = <span class="code-function">cv2.getStructuringElement</span>(cv2.MORPH_RECT, (5, 5))

<span class="code-comment"># Dilation: Expand white regions (connect broken text)</span>
morph = <span class="code-function">cv2.dilate</span>(thresh, kernel, iterations=2)

<span class="code-comment"># Erosion: Shrink white regions (remove noise)</span>
morph = <span class="code-function">cv2.erode</span>(morph, kernel, iterations=1)

<span class="code-comment"># Result: Clean binary image with connected text</span>
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Morphological Operations:</strong> Non-linear image processing</li>
                            <li><strong>Structuring Elements:</strong> Kernels that define operation shape</li>
                            <li><strong>Dilation vs Erosion:</strong> Expand vs shrink, when to use each</li>
                            <li><strong>Opening & Closing:</strong> Compound operations for specific effects</li>
                            <li><strong>Noise Removal:</strong> Practical techniques for cleaning images</li>
                            <li><strong>Text Enhancement:</strong> Connecting broken characters</li>
                        </ul>
                    </div>
                </div>

                <!-- Hierarchical Contour Analysis -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üå≥ Concept: Hierarchical Contour Analysis</div>
                    <div class="concept-subtitle">Understanding parent-child relationships in shapes (holes in letters)</div>

                    <p><strong>The Problem:</strong> How to distinguish text (has holes like "O", "A") from solid images?</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Analyze contour hierarchy - text often has "child" contours (holes).</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">RETR_TREE</div>
                            <div class="term-description">
                                Retrieval mode that captures full hierarchy.
                                <br><br>
                                Unlike RETR_EXTERNAL (only outer), this finds nested contours
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Hierarchy Array</div>
                            <div class="term-description">
                                [Next, Previous, First_Child, Parent]
                                <br><br>
                                Describes relationships between contours
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Parent Contour</div>
                            <div class="term-description">
                                Outer boundary of a shape.
                                <br><br>
                                Example: Outer edge of letter "O"
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Child Contour</div>
                            <div class="term-description">
                                Hole inside a parent shape.
                                <br><br>
                                Example: Inner hole of letter "O"
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">First_Child Index</div>
                            <div class="term-description">
                                Index of first child contour (-1 if no children).
                                <br><br>
                                If != -1, this shape has holes
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Hole Count</div>
                            <div class="term-description">
                                Number of child contours (holes).
                                <br><br>
                                Text often has multiple holes (A, O, D, etc.)
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Find contours WITH hierarchy</span>
contours, hierarchy = <span class="code-function">cv2.findContours</span>(
    morph,
    cv2.RETR_TREE,              <span class="code-comment"># Capture full hierarchy</span>
    cv2.CHAIN_APPROX_SIMPLE
)

<span class="code-comment"># Analyze hierarchy for each contour</span>
<span class="code-keyword">for</span> idx, contour <span class="code-keyword">in</span> <span class="code-function">enumerate</span>(contours):
    <span class="code-comment"># Get hierarchy info: [Next, Previous, First_Child, Parent]</span>
    h_info = hierarchy[0][idx]
    first_child = h_info[2]  <span class="code-comment"># Index of first child</span>

    <span class="code-keyword">if</span> first_child != -1:
        <span class="code-comment"># This contour has holes (children)</span>
        has_holes = <span class="code-keyword">True</span>

        <span class="code-comment"># Count all children (siblings of first child)</span>
        hole_count = 0
        child_idx = first_child
        <span class="code-keyword">while</span> child_idx != -1:
            hole_count += 1
            child_idx = hierarchy[0][child_idx][0]  <span class="code-comment"># Next sibling</span>
                    </div>

                    <div class="key-insight">
                        <div class="key-insight-title">üí° Text vs Image Heuristic</div>
                        <strong>Observation:</strong> Letters often have holes (O, A, D, P, Q, R, B, etc.)<br>
                        <strong>Heuristic:</strong> Shapes with holes are more likely to be text<br>
                        <strong>Score:</strong> hole_count √ó 2 points towards "text" classification
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Tree Structures:</strong> Parent-child relationships in data</li>
                            <li><strong>Hierarchy Representation:</strong> Array format [Next, Prev, Child, Parent]</li>
                            <li><strong>Topology:</strong> Understanding holes and nested shapes</li>
                            <li><strong>Traversal Algorithms:</strong> Walking through siblings</li>
                            <li><strong>Feature Engineering:</strong> Extracting meaningful features (hole count)</li>
                            <li><strong>Heuristic Design:</strong> Using observations to build rules</li>
                        </ul>
                    </div>
                </div>

                <!-- Color Analysis -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üé® Concept: HSV Color Space Analysis</div>
                    <div class="concept-subtitle">Using color to distinguish text (monochrome) from images (colorful)</div>

                    <p><strong>The Problem:</strong> Text is usually black/white/single color. Images are colorful.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Analyze saturation (colorfulness) in HSV color space.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">BGR Color Space</div>
                            <div class="term-description">
                                OpenCV's default: Blue, Green, Red channels.
                                <br><br>
                                Good for display, bad for color analysis
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">HSV Color Space</div>
                            <div class="term-description">
                                Hue, Saturation, Value (brightness).
                                <br><br>
                                Better for color analysis - separates color from brightness
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Hue (H)</div>
                            <div class="term-description">
                                The color itself (0-179 in OpenCV).
                                <br><br>
                                Red, Blue, Green, etc.
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Saturation (S)</div>
                            <div class="term-description">
                                How "colorful" vs "gray" (0-255).
                                <br><br>
                                0 = grayscale, 255 = pure color
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Value (V)</div>
                            <div class="term-description">
                                Brightness (0-255).
                                <br><br>
                                0 = black, 255 = bright
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Standard Deviation</div>
                            <div class="term-description">
                                Measures variance in pixel values.
                                <br><br>
                                High std = varied colors (image)<br>
                                Low std = uniform (text)
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Extract region from book cover</span>
region_crop = book_image[y:y+h, x:x+w]

<span class="code-comment"># Convert to HSV color space</span>
region_hsv = <span class="code-function">cv2.cvtColor</span>(region_crop, cv2.COLOR_BGR2HSV)

<span class="code-comment"># Analyze saturation channel (colorfulness)</span>
saturation_mean = np.<span class="code-function">mean</span>(region_hsv[:, :, 1])  <span class="code-comment"># S channel</span>

<span class="code-comment"># Calculate color variance (how varied the colors are)</span>
color_std = np.<span class="code-function">std</span>(region_crop)

<span class="code-comment"># Determine if monochrome (text) or colorful (image)</span>
is_monochrome = (saturation_mean &lt; 30 <span class="code-keyword">and</span> color_std &lt; 40)

<span class="code-comment"># Add to text score if monochrome</span>
<span class="code-keyword">if</span> is_monochrome:
    text_score += 3  <span class="code-comment"># Monochrome adds to text likelihood</span>
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Color Spaces:</strong> BGR vs HSV, when to use each</li>
                            <li><strong>HSV Advantages:</strong> Separating color from brightness</li>
                            <li><strong>Saturation Analysis:</strong> Measuring "colorfulness"</li>
                            <li><strong>Statistical Measures:</strong> Mean, standard deviation</li>
                            <li><strong>Color Variance:</strong> Using std to detect uniform regions</li>
                            <li><strong>Multi-Signal Classification:</strong> Combining color + shape features</li>
                        </ul>
                    </div>
                </div>

                <!-- Multi-Factor Scoring -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üìä Concept: Multi-Factor Text Scoring</div>
                    <div class="concept-subtitle">Combining multiple weak signals into strong classification</div>

                    <p><strong>The Problem:</strong> No single feature perfectly identifies text vs images.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Combine multiple features with weights to create a "text likelihood score".</p>

                    <div class="code-block">
<span class="code-comment"># Calculate text likelihood score (0-10+)</span>
text_score = 0

<span class="code-comment"># Factor 1: Holes (strong indicator of text)</span>
text_score += hole_count * 2  <span class="code-comment"># 2 points per hole</span>

<span class="code-comment"># Factor 2: Monochrome (text is usually one color)</span>
<span class="code-keyword">if</span> is_monochrome:
    text_score += 3  <span class="code-comment"># 3 points for low saturation</span>

<span class="code-comment"># Factor 3: Aspect ratio (wide = text lines)</span>
<span class="code-keyword">if</span> aspect_ratio &gt; 2:  <span class="code-comment"># Width &gt; 2√ó height</span>
    text_score += 2  <span class="code-comment"># 2 points for horizontal</span>

<span class="code-comment"># Classification based on score</span>
<span class="code-keyword">if</span> text_score &gt;= 5:
    classification = <span class="code-string">"Text Block"</span>
<span class="code-keyword">elif</span> text_score &gt;= 3:
    classification = <span class="code-string">"Possible Text"</span>
<span class="code-keyword">else</span>:
    classification = <span class="code-string">"Image/Icon"</span>
                    </div>

                    <div class="key-insight">
                        <div class="key-insight-title">üí° Ensemble Learning Principle</div>
                        <strong>Key Insight:</strong> Multiple weak classifiers ‚Üí one strong classifier<br><br>
                        <strong>Features Used:</strong>
                        <ul style="margin-left: 25px; margin-top: 10px;">
                            <li>Hole count (topology)</li>
                            <li>Color saturation (appearance)</li>
                            <li>Aspect ratio (geometry)</li>
                            <li>Area percentage (size)</li>
                        </ul>
                        <br>
                        <strong>Result:</strong> More robust than any single feature alone
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Feature Engineering:</strong> Extracting meaningful features from raw data</li>
                            <li><strong>Weighted Scoring:</strong> Different features have different importance</li>
                            <li><strong>Ensemble Methods:</strong> Combining multiple classifiers</li>
                            <li><strong>Threshold-Based Classification:</strong> Score ranges ‚Üí classes</li>
                            <li><strong>Heuristic Weights:</strong> Designing weights based on observation</li>
                            <li><strong>Robustness:</strong> Multiple features reduce error</li>
                        </ul>
                    </div>
                </div>

                <!-- OCR Integration -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">ü§ñ Concept: OCR Integration (EasyOCR)</div>
                    <div class="concept-subtitle">Using deep learning to READ the detected text regions</div>

                    <p><strong>The Problem:</strong> We detected WHERE text is, but not WHAT it says.</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Use EasyOCR neural network to recognize characters.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">EasyOCR</div>
                            <div class="term-description">
                                Deep learning library for text recognition.
                                <br><br>
                                Pre-trained on millions of text images
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Neural Network</div>
                            <div class="term-description">
                                Multi-layer model trained to recognize characters.
                                <br><br>
                                Learns patterns from data, not hand-coded rules
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Reader Object</div>
                            <div class="term-description">
                                EasyOCR interface for text recognition.
                                <br><br>
                                <strong>['en']:</strong> English language model
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">GPU vs CPU</div>
                            <div class="term-description">
                                Neural networks run faster on GPUs.
                                <br><br>
                                <strong>gpu=False:</strong> Use CPU (slower but works everywhere)
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Confidence Score</div>
                            <div class="term-description">
                                How certain the model is (0.0 - 1.0).
                                <br><br>
                                0.9+ = very confident<br>
                                &lt;0.5 = uncertain
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Detail Parameter</div>
                            <div class="term-description">
                                detail=1: Returns bbox, text, confidence<br>
                                detail=0: Returns only text
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Initialize EasyOCR (one-time setup)</span>
<span class="code-keyword">import</span> easyocr
reader = easyocr.<span class="code-function">Reader</span>([<span class="code-string">'en'</span>], gpu=<span class="code-keyword">False</span>)

<span class="code-comment"># Run OCR on each detected text region</span>
<span class="code-keyword">for</span> region <span class="code-keyword">in</span> text_regions:
    x, y, w, h = region[<span class="code-string">'bbox'</span>]

    <span class="code-comment"># Crop region from book image</span>
    region_crop = book_image[y:y+h, x:x+w]

    <span class="code-comment"># Run OCR (neural network inference)</span>
    result = reader.<span class="code-function">readtext</span>(region_crop, detail=1)

    <span class="code-comment"># Result format: [(bbox, text, confidence), ...]</span>
    <span class="code-keyword">if</span> result:
        text = result[0][1]        <span class="code-comment"># Extract text</span>
        confidence = result[0][2]  <span class="code-comment"># Extract confidence</span>

        <span class="code-function">print</span>(<span class="code-string">f"Found: {text} (confidence: {confidence:.2f})"</span>)
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Deep Learning:</strong> Neural networks for pattern recognition</li>
                            <li><strong>Pre-trained Models:</strong> Using models trained on massive datasets</li>
                            <li><strong>Transfer Learning:</strong> Applying pre-trained models to your task</li>
                            <li><strong>OCR Technology:</strong> How computers read text from images</li>
                            <li><strong>Confidence Scores:</strong> Quantifying model certainty</li>
                            <li><strong>Hybrid Pipelines:</strong> Classical CV (detection) + DL (recognition)</li>
                        </ul>
                    </div>
                </div>

                <!-- Results & Limitations -->
                <div class="results-box">
                    <div class="results-title">üìà Path A Results: Manual Detection</div>

                    <div class="stat-grid">
                        <div class="stat-item">
                            <div class="stat-number">43</div>
                            <div class="stat-label">Regions Detected</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">28</div>
                            <div class="stat-label">Text Regions</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">15</div>
                            <div class="stat-label">Image Regions</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">17</div>
                            <div class="stat-label">Readable Text</div>
                        </div>
                    </div>

                    <div class="key-insight" style="margin-top: 20px;">
                        <div class="key-insight-title">‚ö†Ô∏è Critical Limitation Discovered</div>
                        <strong>The "Letter Splitting" Problem:</strong><br><br>

                        <strong>What Happened:</strong> The word "ATOMIC" was detected as separate letters: A, T, O, M, I, C<br><br>

                        <strong>Why It Happened:</strong> Contour detection treats each letter as a separate shape<br><br>

                        <strong>Impact:</strong> OCR struggles with single letters (needs context of whole words)<br><br>

                        <strong>Example Results:</strong>
                        <ul style="margin-left: 25px; margin-top: 10px;">
                            <li>Region #5: "M" (should be part of "ATOMIC")</li>
                            <li>Region #7: "A" (should be part of "ATOMIC")</li>
                            <li>Region #12: "S" (should be part of "SCALING")</li>
                            <li>Region #13: "C" (should be part of "SCALING")</li>
                        </ul>
                        <br>
                        <strong>This limitation led us to explore Path B...</strong>
                    </div>
                </div>

                <div class="image-placeholder">
                    üì∑ Image 3: Manual Detection Result
                    <br>Shows 43 bounding boxes with many single-letter detections
                </div>
            </div>

            <!-- PATH B: OCR-FIRST APPROACH -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">2B</div>
                    <span>Path B: OCR-First Approach (Deep Learning)</span>
                </div>

                <div class="stage-badge">APPROACH 2: USE PRE-TRAINED AI</div>

                <div class="concept-box">
                    <div class="concept-title">üöÄ Concept: End-to-End OCR</div>
                    <div class="concept-subtitle">Let the neural network detect AND read text in one step</div>

                    <p><strong>The Insight:</strong> Why build manual detection when EasyOCR can already find text?</p>

                    <p style="margin-top: 15px;"><strong>New Approach:</strong> Skip manual detection. Run OCR on the entire book cover.</p>

                    <div class="key-insight">
                        <div class="key-insight-title">üß† Paradigm Shift</div>
                        <strong>Old Approach (Path A):</strong><br>
                        Manual Detection ‚Üí Crop Regions ‚Üí OCR Each Region<br>
                        (3 stages, complex, splits words)<br><br>

                        <strong>New Approach (Path B):</strong><br>
                        Full-Image OCR ‚Üí Get Complete Words<br>
                        (1 stage, simple, keeps words together)
                    </div>

                    <div class="code-block">
<span class="code-comment"># OLD APPROACH (Path A): Manual detection first</span>
edges = <span class="code-function">cv2.Canny</span>(...)
contours = <span class="code-function">cv2.findContours</span>(...)
<span class="code-keyword">for</span> contour <span class="code-keyword">in</span> contours:
    region = crop(contour)
    text = reader.<span class="code-function">readtext</span>(region)  <span class="code-comment"># OCR single letters</span>

<span class="code-comment"># NEW APPROACH (Path B): OCR does everything</span>
results = reader.<span class="code-function">readtext</span>(book_image, detail=1)

<span class="code-comment"># Results: [(bbox, text, confidence), ...]</span>
<span class="code-comment"># EasyOCR finds text regions AND reads them!</span>
<span class="code-comment"># Returns complete words: "ATOMIC", "SCALING", "TEAM"</span>
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>End-to-End Learning:</strong> Neural networks that do multiple tasks</li>
                            <li><strong>Text Detection:</strong> Modern OCR finds text automatically</li>
                            <li><strong>Simplification:</strong> When to remove complexity for better results</li>
                            <li><strong>Pre-trained Models:</strong> Leveraging models trained on millions of images</li>
                            <li><strong>Bottom-Up vs Top-Down:</strong> Building features vs learned features</li>
                        </ul>
                    </div>
                </div>

                <!-- Text Region Grouping -->
                <div class="concept-box" style="margin-top: 30px;">
                    <div class="concept-title">üîó Concept: Text Region Grouping</div>
                    <div class="concept-subtitle">Merging nearby text boxes that belong together</div>

                    <p><strong>The Problem:</strong> OCR sometimes returns nearby words as separate boxes ("How", "Small", "Teams").</p>

                    <p style="margin-top: 15px;"><strong>The Solution:</strong> Group text boxes that are on the same line and close together.</p>

                    <div class="terms-grid">
                        <div class="term-card">
                            <div class="term-name">Spatial Proximity</div>
                            <div class="term-description">
                                How close two objects are in space.
                                <br><br>
                                Measure distance between bounding boxes
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Same Line Detection</div>
                            <div class="term-description">
                                Check if boxes have similar Y coordinates.
                                <br><br>
                                <strong>Threshold:</strong> Within 50% of height
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Horizontal Distance</div>
                            <div class="term-description">
                                Gap between boxes horizontally.
                                <br><br>
                                Small gap = likely same phrase
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Height Similarity</div>
                            <div class="term-description">
                                Check if boxes have similar height (same font).
                                <br><br>
                                <strong>Threshold:</strong> Within 30% difference
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Bounding Box Union</div>
                            <div class="term-description">
                                Create new box that contains all grouped boxes.
                                <br><br>
                                min(x), min(y), max(x+w), max(y+h)
                            </div>
                        </div>

                        <div class="term-card">
                            <div class="term-name">Text Concatenation</div>
                            <div class="term-description">
                                Join text from grouped boxes.
                                <br><br>
                                "How" + "Small" + "Teams" = "How Small Teams"
                            </div>
                        </div>
                    </div>

                    <div class="code-block">
<span class="code-comment"># Sort text boxes by position (top to bottom, left to right)</span>
sorted_regions = <span class="code-function">sorted</span>(text_regions,
                         key=<span class="code-keyword">lambda</span> r: (r[<span class="code-string">'center_y'</span>], r[<span class="code-string">'center_x'</span>]))

<span class="code-comment"># Group nearby boxes</span>
current_group = [sorted_regions[0]]

<span class="code-keyword">for</span> i <span class="code-keyword">in</span> <span class="code-function">range</span>(1, <span class="code-function">len</span>(sorted_regions)):
    current = sorted_regions[i]
    previous = sorted_regions[i-1]

    <span class="code-comment"># Calculate distances</span>
    y_distance = <span class="code-function">abs</span>(current[<span class="code-string">'center_y'</span>] - previous[<span class="code-string">'center_y'</span>])
    x_distance = current[<span class="code-string">'bbox'</span>][0] - (previous[<span class="code-string">'bbox'</span>][0] + previous[<span class="code-string">'bbox'</span>][2])
    height_diff = <span class="code-function">abs</span>(current[<span class="code-string">'bbox'</span>][3] - previous[<span class="code-string">'bbox'</span>][3])

    <span class="code-comment"># Check grouping criteria</span>
    same_line = y_distance &lt; previous[<span class="code-string">'bbox'</span>][3] * 0.5
    close_together = x_distance &lt; previous[<span class="code-string">'bbox'</span>][2] * 1.5
    similar_height = height_diff &lt; previous[<span class="code-string">'bbox'</span>][3] * 0.3

    <span class="code-keyword">if</span> same_line <span class="code-keyword">and</span> close_together <span class="code-keyword">and</span> similar_height:
        current_group.<span class="code-function">append</span>(current)  <span class="code-comment"># Add to group</span>
    <span class="code-keyword">else</span>:
        <span class="code-function">merge_group</span>(current_group)  <span class="code-comment"># Merge previous group</span>
        current_group = [current]     <span class="code-comment"># Start new group</span>
                    </div>

                    <div class="results-box">
                        <div class="results-title">‚úÖ Grouping Results</div>
                        <p style="margin-top: 10px;"><strong>Before Grouping:</strong> 7 separate text regions</p>
                        <ul style="margin-left: 25px; margin-top: 10px;">
                            <li>"AToMIC"</li>
                            <li>"SCALING"</li>
                            <li>"How"</li>
                            <li>"Small"</li>
                            <li>"Teams"</li>
                            <li>"Create Huge Growth"</li>
                            <li>"LUDOVIC BODIN"</li>
                        </ul>

                        <p style="margin-top: 15px;"><strong>After Grouping:</strong> 5 merged regions</p>
                        <ul style="margin-left: 25px; margin-top: 10px;">
                            <li>"AToMIC"</li>
                            <li>"SCALING"</li>
                            <li>"How Small Teams" ‚Üê <strong>Merged from 3 boxes!</strong></li>
                            <li>"Create Huge Growth"</li>
                            <li>"LUDOVIC BODIN"</li>
                        </ul>
                    </div>

                    <div class="what-you-learned">
                        <h3>üéì What You Learned</h3>
                        <ul>
                            <li><strong>Spatial Algorithms:</strong> Proximity-based grouping</li>
                            <li><strong>Clustering Basics:</strong> Grouping similar items</li>
                            <li><strong>Multi-Criteria Decisions:</strong> AND logic (all conditions must be true)</li>
                            <li><strong>Bounding Box Operations:</strong> Union, merge, expansion</li>
                            <li><strong>Text Processing:</strong> Concatenation with spacing</li>
                            <li><strong>Pipeline Design:</strong> Post-processing to improve results</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- COMPARISON & ANALYSIS -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">‚öñÔ∏è</div>
                    <span>Path A vs Path B: Complete Analysis</span>
                </div>

                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Path A: Manual Detection</th>
                            <th>Path B: OCR-First</th>
                            <th>Winner</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Regions Detected</strong></td>
                            <td>43 regions</td>
                            <td>7 ‚Üí 5 (after grouping)</td>
                            <td><span class="pro">Path B</span> (less noise)</td>
                        </tr>
                        <tr>
                            <td><strong>Word Integrity</strong></td>
                            <td><span class="con">‚úó</span> Splits "ATOMIC" into A,T,O,M,I,C</td>
                            <td><span class="pro">‚úì</span> Keeps "ATOMIC" as one word</td>
                            <td><span class="pro">Path B</span></td>
                        </tr>
                        <tr>
                            <td><strong>OCR Accuracy</strong></td>
                            <td>~40% (struggles with single letters)</td>
                            <td>~95% (reads complete words)</td>
                            <td><span class="pro">Path B</span></td>
                        </tr>
                        <tr>
                            <td><strong>Pipeline Complexity</strong></td>
                            <td>8 steps (threshold, morph, contours, hierarchy, color, scoring, OCR)</td>
                            <td>2 steps (OCR, grouping)</td>
                            <td><span class="pro">Path B</span> (simpler)</td>
                        </tr>
                        <tr>
                            <td><strong>Learning Value</strong></td>
                            <td><span class="pro">‚úì‚úì‚úì</span> Deep understanding of CV fundamentals</td>
                            <td><span class="pro">‚úì</span> Understanding modern AI tools</td>
                            <td><span class="pro">Path A</span> (education)</td>
                        </tr>
                        <tr>
                            <td><strong>Parameter Tuning</strong></td>
                            <td>15+ parameters to tune</td>
                            <td>3 parameters (grouping thresholds)</td>
                            <td><span class="pro">Path B</span> (easier)</td>
                        </tr>
                        <tr>
                            <td><strong>Flexibility</strong></td>
                            <td><span class="pro">‚úì</span> Full control over every step</td>
                            <td><span class="con">‚úó</span> Limited control (black box)</td>
                            <td><span class="pro">Path A</span></td>
                        </tr>
                        <tr>
                            <td><strong>Processing Speed</strong></td>
                            <td>~3-5 seconds</td>
                            <td>~2-3 seconds</td>
                            <td><span class="pro">Path B</span> (faster)</td>
                        </tr>
                        <tr>
                            <td><strong>Production Ready</strong></td>
                            <td><span class="con">‚úó</span> Needs major fixes</td>
                            <td><span class="pro">‚úì</span> Works out of the box</td>
                            <td><span class="pro">Path B</span></td>
                        </tr>
                        <tr>
                            <td><strong>Understanding "Why"</strong></td>
                            <td><span class="pro">‚úì‚úì‚úì</span> You built everything, understand each step</td>
                            <td><span class="con">‚úó</span> Neural network is a black box</td>
                            <td><span class="pro">Path A</span></td>
                        </tr>
                    </tbody>
                </table>

                <div class="key-insight" style="margin-top: 30px;">
                    <div class="key-insight-title">üéØ The Meta-Lesson: When to Build vs When to Use</div>

                    <strong>For Learning:</strong> Path A (Manual Detection) is invaluable
                    <ul style="margin-left: 25px; margin-top: 10px;">
                        <li>Teaches fundamental computer vision concepts</li>
                        <li>Builds intuition for how CV algorithms work</li>
                        <li>Helps you understand WHAT pre-trained models are doing</li>
                        <li>Makes you appreciate modern deep learning tools</li>
                    </ul>

                    <br>
                    <strong>For Production:</strong> Path B (OCR-First) is superior
                    <ul style="margin-left: 25px; margin-top: 10px;">
                        <li>Simpler, faster, more accurate</li>
                        <li>Leverages years of research and training</li>
                        <li>Less maintenance (fewer parameters to tune)</li>
                        <li>Battle-tested on millions of images</li>
                    </ul>

                    <br>
                    <strong>The Ideal Approach:</strong> Learn with Path A, deploy with Path B
                    <ul style="margin-left: 25px; margin-top: 10px;">
                        <li>Build from scratch to understand fundamentals</li>
                        <li>Then use modern tools intelligently because you know how they work</li>
                        <li>Fall back to manual techniques when deep learning fails</li>
                        <li>Hybrid approaches often work best</li>
                    </ul>
                </div>
            </div>

            <!-- KEY TAKEAWAYS -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">üíé</div>
                    <span>Key Takeaways & Learnings</span>
                </div>

                <div class="timeline">
                    <div class="timeline-item">
                        <div class="concept-title">1. Classical CV Fundamentals are Essential</div>
                        <p>Even in the age of deep learning, understanding edge detection, thresholding, morphology, and contours gives you:</p>
                        <ul style="margin-left: 20px; margin-top: 10px;">
                            <li>Intuition for what neural networks are learning</li>
                            <li>Debugging skills when AI fails</li>
                            <li>Ability to build hybrid systems</li>
                            <li>Foundation for advanced CV concepts</li>
                        </ul>
                    </div>

                    <div class="timeline-item">
                        <div class="concept-title">2. Feature Engineering Still Matters</div>
                        <p>Combining multiple weak signals (holes, color, aspect ratio) into strong classification teaches you:</p>
                        <ul style="margin-left: 20px; margin-top: 10px;">
                            <li>Ensemble learning principles</li>
                            <li>How to design robust systems</li>
                            <li>Feature importance and weighting</li>
                            <li>Multi-criteria decision making</li>
                        </ul>
                    </div>

                    <div class="timeline-item">
                        <div class="concept-title">3. Pre-trained Models are Powerful</div>
                        <p>EasyOCR demonstrates the value of transfer learning:</p>
                        <ul style="margin-left: 20px; margin-top: 10px;">
                            <li>Trained on millions of images (you can't match this)</li>
                            <li>Handles edge cases you haven't thought of</li>
                            <li>Continuously improved by community</li>
                            <li>Use them! Don't reinvent the wheel</li>
                        </ul>
                    </div>

                    <div class="timeline-item">
                        <div class="concept-title">4. Simpler Can Be Better</div>
                        <p>Path B's success shows that:</p>
                        <ul style="margin-left: 20px; margin-top: 10px;">
                            <li>Fewer steps = fewer failure points</li>
                            <li>End-to-end learning often outperforms pipelines</li>
                            <li>Don't over-engineer when simple solutions exist</li>
                            <li>But you need Path A knowledge to know when to simplify!</li>
                        </ul>
                    </div>

                    <div class="timeline-item">
                        <div class="concept-title">5. Every Approach Has Trade-offs</div>
                        <p>There's no "perfect" solution:</p>
                        <ul style="margin-left: 20px; margin-top: 10px;">
                            <li>Manual: Control vs Complexity</li>
                            <li>Deep Learning: Accuracy vs Interpretability</li>
                            <li>Hybrid: Best of both vs More moving parts</li>
                            <li>Choose based on your constraints (time, accuracy, explainability)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- CONCEPTS LEARNED SUMMARY -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">üìö</div>
                    <span>Complete Concepts Learned (25 Total)</span>
                </div>

                <div class="stat-grid">
                    <div class="stat-item">
                        <div class="stat-number">9</div>
                        <div class="stat-label">Classical CV Techniques</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">7</div>
                        <div class="stat-label">Feature Engineering Methods</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">5</div>
                        <div class="stat-label">Deep Learning Concepts</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">4</div>
                        <div class="stat-label">Software Engineering Practices</div>
                    </div>
                </div>

                <div class="terms-grid" style="margin-top: 30px;">
                    <div class="term-card">
                        <div class="term-name">Classical CV (9)</div>
                        <div class="term-description">
                            ‚Ä¢ Edge detection (Canny)<br>
                            ‚Ä¢ Contour finding<br>
                            ‚Ä¢ Adaptive thresholding<br>
                            ‚Ä¢ Morphological operations<br>
                            ‚Ä¢ Hierarchical analysis<br>
                            ‚Ä¢ Color space analysis (HSV)<br>
                            ‚Ä¢ Perspective transforms<br>
                            ‚Ä¢ Gaussian blur<br>
                            ‚Ä¢ Shape approximation
                        </div>
                    </div>

                    <div class="term-card">
                        <div class="term-name">Feature Engineering (7)</div>
                        <div class="term-description">
                            ‚Ä¢ Aspect ratio analysis<br>
                            ‚Ä¢ Hole detection (topology)<br>
                            ‚Ä¢ Saturation analysis<br>
                            ‚Ä¢ Color variance (std)<br>
                            ‚Ä¢ Multi-factor scoring<br>
                            ‚Ä¢ Weighted combinations<br>
                            ‚Ä¢ Threshold-based classification
                        </div>
                    </div>

                    <div class="term-card">
                        <div class="term-name">Deep Learning (5)</div>
                        <div class="term-description">
                            ‚Ä¢ Pre-trained models (EasyOCR)<br>
                            ‚Ä¢ Neural networks basics<br>
                            ‚Ä¢ Transfer learning<br>
                            ‚Ä¢ Confidence scores<br>
                            ‚Ä¢ End-to-end learning
                        </div>
                    </div>

                    <div class="term-card">
                        <div class="term-name">Software Engineering (4)</div>
                        <div class="term-description">
                            ‚Ä¢ Pipeline architecture<br>
                            ‚Ä¢ Hybrid system design<br>
                            ‚Ä¢ Performance comparison<br>
                            ‚Ä¢ Trade-off analysis
                        </div>
                    </div>
                </div>
            </div>

            <!-- WHAT'S NEXT -->
            <div class="section">
                <div class="section-title">
                    <div class="section-number">üöÄ</div>
                    <span>What's Next: Continuing from Checkpoint A</span>
                </div>

                <div class="concept-box">
                    <div class="concept-title">üìç Current Status: Checkpoint A</div>
                    <p style="font-size: 16px; line-height: 1.8;">
                        You've completed the fundamental CV learning journey and built a working book cover analyzer.
                        The V3 analyzer (OCR-first approach) is implemented but we haven't walked through it step-by-step yet.
                    </p>

                    <div class="key-insight" style="margin-top: 20px;">
                        <div class="key-insight-title">üéØ Next Learning Steps</div>
                        <strong>We still need to explore in detail:</strong>
                        <br><br>
                        <strong>1. Position-Based Classification</strong> - Using layout zones (top/middle/bottom)<br>
                        <strong>2. Size-Based Classification</strong> - Visual hierarchy analysis<br>
                        <strong>3. Centrality Scoring</strong> - Detecting centered elements (titles)<br>
                        <strong>4. Multi-Factor Scoring</strong> - Combining position + size + centrality<br>
                        <strong>5. Pattern Matching (Regex)</strong> - Detecting author names, ISBNs<br>
                        <strong>6. Decision Fusion</strong> - Final classification logic<br>
                        <br>
                        <strong>Status:</strong> Code is written, concepts not yet taught in detail
                    </div>
                </div>
            </div>

            <!-- FOOTER -->
            <div style="text-align: center; padding: 40px; background: #f8f9fa; border-radius: 12px; margin-top: 40px;">
                <h2 style="color: #667eea; margin-bottom: 20px;">üéì Learning Journey Complete (So Far)</h2>
                <p style="font-size: 18px; color: #555; line-height: 1.8;">
                    You've learned the fundamentals of computer vision through hands-on practice,<br>
                    understood the trade-offs between manual and AI-based approaches,<br>
                    and built a working book cover analyzer.
                </p>
                <p style="font-size: 16px; color: #666; margin-top: 20px;">
                    <strong>From Checkpoint A, we continue with advanced classification techniques...</strong>
                </p>
            </div>

        </div>
    </div>
</body>
</html>